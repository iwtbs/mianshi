# 过拟合
过拟合是方差大，欠拟合是偏差大
## 过拟合的原因和解决方法
- 产生过拟合原因：
参数太多，模型复杂度高
样本中噪音数据较大，模型学习到了噪音中的特征
对于决策树模型。对决策树的生长没有合理的限制和修建
对于神经网络模型。权值学习迭代次数足够多（overtraining），拟合了训练数据中的噪声和训练样例中没有代表性的特征。

- 解决方案：
降低模型复杂度
增大训练集，训练集增加之后就能学习到更多具有代表性的特征
增加正则项，减少参数，进一步降低模型复杂度
对于神经网络，采用dropout
对于决策树，采用earlystopping，模型对训练数据集迭代收敛之前停止，防止过拟合
采用ensemble。集成学习可以有效的减轻过拟合。bagging通过平均多个模型的结果，来降低模型的方差。boosting不仅可以减小偏差，还可以减小方差。

## 什么是正则化？简述一下范数的意义是？
正则化就是结构风险最小化策略的实现，是在经验风险最小化的情况下加入一个正则化项或者罚项。
范数是一种用来度量某个向量空间（或矩阵）中的每个向量的长度或大小的手段。
## L1,L2正则化的原理和区别？为什么L1正则化会产生稀疏解而L2正则化会产生平滑解？
正则化是结构风险最小化策略的实现，L1和L2正则化属于正则化手段中的两种实现方式，L1正则化是在损失函数中加入 参数向量中各个元素的绝对值之和作为损失函数的惩罚项的，L2正则化是在损失函数中加入 参数向量中各个元素的平方，求和，然后再求平方根作为损失函数的惩罚项的。这就是二者的原理与区别。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106130349752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
L1正则化代表的约束条件的多维空间是超立方体和坐标轴存在很多“角”交点，目标函数大部分时候会在“角”的地方和约束条件相交，所以L1正则化容易产生稀疏的参数向量，而L2正则化是一个超球体，因为没有“角”交点，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了，所以L2正则化容易产生平滑的参数向量。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106130515766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近根据其梯度，L1的下降速度比L2的下降速度要快
## L1和L2除了正则化防止过拟合还有什么作用
防止过拟合的具体表现：**在不显著增大偏差的的同时，显著减小模型的方差**

L1正则化除了防止过拟合还可以作为特征筛选的方法，使得对模型不是太重要的特征的权重系数趋于0，那么我们就可以根据具体情况来对特征进行删除和重选择，从而起到提高泛化性能以及节约内存空间，提升运行效率的作用。

## L1正则（lasso回归）不是连续可导的，那么还能用梯度下降么，如果不能的话如何优化求解
由于lasso回归的损失函数是不可导的，所以梯度下降算法将不再有效，下面利用坐标轴下降法进行求解。

坐标轴下降法和梯度下降法具有同样的思想，都是沿着某个方向不断迭代，但是梯度下降法是沿着当前点的负梯度方向进行参数更新，而坐标轴下降法是沿着坐标轴的方向

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106131754174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
 1. 坐标轴下降法进行参数更新时，每次总是固定另外m-1个值，求另外一个的局部最优值，这样也避免了Lasso回归的损失函数不可导的问题。

2. 坐标轴下降法每轮迭代都需要O(mn)的计算。（和梯度下降算法相同）

## Ridge和Lasso的实现，他们的区别是什么？分别是如何求解的？
Ridge=线性回归+L2正则，有闭合解；Lasso=线性回归+L1正则，无闭合解，可用坐标梯度下降求解
## Dropout的原理 (为什么训练时有dropout测试时没有dropout，这样会发生scale的偏移吗)
不会，因为训练时dropout机制会把dropout_rate = p的输出乘以1/(1-p)