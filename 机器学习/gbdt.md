# GBDT
[gbdt实际例子](https://mp.weixin.qq.com/s/adMMz8u29OnvqelgZuXWjg)
## gbdt中的树是回归树还是分类树？
回归树

这里面的核心是因为GBDT 每轮的训练是在上一轮的训练的残差基础之上进行训练的。
这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。而类别结果相减是无意义的，因此需要数值结果进行相减，所以使用CART 回归树。

当CART是分类树时，采用GINI值作为节点分裂的依据；当CART是回归树时，采用样本的最小方差作为节点分裂的依据
[CART](https://blog.csdn.net/beauty0522/article/details/82726866)
## 简述gbdt 的算法的流程
[gbdt实际例子](https://blog.csdn.net/zpalyq110/article/details/79527653)
GBDT的核心就在于，每一棵树学的是之前所有树结论和的**残差**，这个残差就是一个加预测值后能得真实值的累加量

比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106133534622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。
对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。

具体到损失函数本身的选择也就是L的选择，有平方损失函数，0-1损失函数，对数损失函数等等。如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差。

GBDT每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。
这样每轮训练的时候都能够让损失函数尽可能快的减小，尽快的收敛达到局部最优解或者全局最优解。

## gbdt 如何选择特征 ？
CART TREE 生成的过程其实就是一个选择特征的过程。假设我们目前总共有 M 个特征。第一步我们需要从中选择出一个特征 j，做为二叉树的第一个节点。然后对特征 j 的值选择一个切分点 m. 一个 样本的特征j的值如果小于m，则分为一类，如果大于m,则分为另外一类。如此便构建了CART 树的一个节点。其他节点的生成过程和这个是一样的
## gbdt 如何用于分类？
具体到分类这个任务上面来，我们假设样本 X 总共有 K类。来了一个样本 x，我们需要使用GBDT来判断 x 属于样本的哪一类。
第一步 我们在训练的时候，是针对样本 X 每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是 K = 3。样本 x 属于 第二类。那么针对该样本 x 的分类结果，其实我们可以用一个 三维向量 [0,1,0] 来表示。0表示样本不属于该类，1表示样本属于该类。由于样本已经属于第二类了，所以第二类对应的向量维度为1，其他位置为0。

针对样本有 三类的情况，我们实质上是在每轮的训练的时候是同时训练三颗树。第一颗树针对样本x的第一类，输入为（x,0）。第二颗树输入针对 样本x 的第二类，输入为（x,1）。第三颗树针对样本x 的第三类，输入为（x，0）

在这里每颗树的训练过程其实就是就是我们之前已经提到过的CATR TREE 的生成过程。在此处我们参照之前的生成树的程序 即可以就解出三颗树，以及三颗树对x 类别的预测值f1(x),f2(x),f3(x)。那么在此类训练中，我们仿照多分类的逻辑回归 ，使用softmax 来产生概率，则属于类别 1 的概率
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106145948718.png)

并且我们我们可以针对类别1 求出 残差y11(x)=0−p1(x);类别2 求出残差y22(x)=1−p2(x);类别3 求出残差y33(x)=0−p3(x).

然后开始第二轮训练 针对第一类 输入为（x,y11(x)）, 针对第二类输入为（x,y22(x)), 针对 第三类输入为 (x,y33(x)).继续训练出三颗树。一直迭代M轮。每轮构建 3颗树。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106150007894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
当训练完毕以后，新来一个样本 x1 ，我们需要预测该样本的类别的时候，便可以有这三个式子产生三个值，f1(x),f2(x),f3(x)。样本属于 某个类别c的概率为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106150020628.png)
## gbdt的效果相比于传统的LR，SVM效果为什么好一些 ？
GBDT基于树模型，继承了树模型的优点 ：对异常点鲁棒、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小
## gbdt的参数有哪些，如何调参 ？
1、首先使用默认的参数，进行数据拟合； 
2、从步长(learning rate)和迭代次数(n_estimators)入手；一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，可以将步长初始值设置为0.1。对于迭代次数进行网格搜索； 
3、接下来对决策树的参数进行寻优 
4、首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。【min_samples_split暂时不能一起定下来，因为这个还和决策树其他的参数存在关联】 
5、接着再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参；做到这里，min_samples_split要做两次网格寻优，一次是树的最大深度max_depth，一次是叶子节点最少样本数min_samples_leaf。 
【具体观察min_samples_split的值是否落在边界上，如果是可以进一步寻优】 
6、继续对最大特征数max_features进行网格搜索。做完这一步可以看看寻找出的最优参数组合给出的分类器的效果。 
7、可以进一步考虑对子采样的比例进行网格搜索，得到subsample的寻优参数 
8、回归到第2步调整设定的步长(learning rate)和迭代次数(n_estimators)，注意两者的乘积保持不变，这里可以分析得到：通过减小步长可以提高泛化能力，但是步长设定过小，也会导致拟合效果反而变差，也就是说，步长不能设置的过小。
[如何调gbdt](https://blog.csdn.net/weixin_33811539/article/details/86229899)
## gbdt的优缺点 ？GBDT优缺点 
A：优点　　 
可以灵活处理各种类型的数据，包括连续值和离散值。 
在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 
使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 
缺点：由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。
## RF和GBDT的区别
相同点：

都是由多棵树组成，最终的结果都是由多棵树一起决定。

不同点：

- 集成学习：RF属于bagging思想，而GBDT是boosting思想

- 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差

- 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本

- 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)

- 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合

- 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感

- 泛化能力：RF不易过拟合，而GBDT容易过拟合

## 比较LR和GBDT，说说什么情景下GBDT不如LR
先说说LR和GBDT的区别：
- LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；

举例：

```bash
假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。

我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。
```
那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1*f1 + Wi*fi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？

仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。

这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合
## 与传统的Boost的区别是
每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别
## GBDT的正则化
1. 在每次对残差估计进行迭代时，不直接加上当前步所拟合的残差，而是乘以一个系数，比如这个0.1。较小的学习率意味着我们需要更多的弱学习器的迭代次数。通常我们用学习率和迭代最大次数一起来决定算法的拟合效果
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224113150183.png)
2. 通过子采样比例（subsample），取值为 (0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。
3. 对于弱学习器即CART回归树进行正则化剪枝
4. Early Stopping是机器学习迭代式训练模型中很常见的防止过拟合技巧，具体的做法是选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，就停止训练，也就是说控制迭代的轮数（树的个数）。在sklearn的GBDT中可以设置参数n_iter_no_change实现early stopping。
## GBDT与AdaBoost的区别与联系？
AdaBoost和GBDT都是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过调整错分数据点的权重来改进模型，GBDT是通过计算负梯度来改进模型。因此，相比AdaBoost, GBDT可以使用更多种类的目标函数，而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。
## gbdt二分类的负梯度怎么算
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224122004372.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
最终计算：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224121850811.png)