# 决策树
## 决策树有哪几种？
决策树算法，无论是哪种，其目的都是为了让模型的不确定性降低的越快越好，基于其评价指标的不同，主要是ID3算法，C4.5算法和CART算法，其中ID3算法的评价指标是信息增益，C4.5算法的评价指标是信息增益率，CART算法的评价指标是基尼系数。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223205728561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)

## ID3算法存在的问题
会倾向于去选择特征取值比较多的特征作为最优特征。举个比较极端的例子，如果将身份证号作为一个属性，这个属性可以完美划分但是毫无意义
信息增益率的公式如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200208232800157.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## 决策树出现过拟合的原因及其解决办法？
原因：
- 在决策树构建的过程中，对决策树的生长没有进行合理的限制（剪枝）；
- 样本中有一些噪声数据，没有对噪声数据进行有效的剔除；
- 在构建决策树过程中使用了较多的输出变量，变量较多也容易产生过拟合
解决办法：
- 选择合理的参数进行剪枝，可以分为预剪枝和后剪枝，我们一般采用后剪枝的方法；
- 减少特征，计算每一个特征和响应变量的相关性，常见得为皮尔逊相关系数，将相关性较小的变量剔除。当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等。

## 简单解释一下预剪枝和后剪枝，以及剪枝过程中可以参考的参数有哪些？
- 预剪枝：在决策树生成初期就已经设置了决策树的参数，决策树构建过程中，满足参数条件就提前停止决策树的生成。
- 后剪枝：后剪枝是一种全局的优化方法，它是在决策树完全建立之后再返回去对决策树进行剪枝。
- 参数：树的高度、叶子节点的数目、最大叶子节点数、限制不纯度。