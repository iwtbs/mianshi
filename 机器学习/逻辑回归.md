# 逻辑回归LR
## 损失公式推导
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200208205659171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## 为什么逻辑回归采用似然函数，而不是平方损失函数？
这个式子的更新速度只和xi,yi相关。和sigmoid函数本身的梯度无关，这样更新的速度是可以自始至终都比较的稳定。至于为什么不用平方损失函数，使用平方损失函数的话梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200208210042822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## 缺点
1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
4. 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归
## LR和SVM的联系与区别
联系： 
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 

区别：
1、LR是参数模型，SVM是非参数模型。 
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200106104213951.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200208210339296.png) 3、支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）
 4、SVM的损失函数就自带正则。 
 5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

## 并行化怎么做，有几种并行化方式，读过哪些开源的实现
将M个样本的标签构成一个M维的标签向量，M个N维特征向量构成一个M*N的样本矩阵，如图3所示。其中特征矩阵每一行为一个特征向量（M行），列为特征维度（N列）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200208210558442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)