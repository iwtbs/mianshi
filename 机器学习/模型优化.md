# 模型优化
## 损失函数有哪些
1. sgd
![](https://img-blog.csdnimg.cn/20200209114820766.png)
2. 动量(Momentum)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209114859491.png)
3. AdaGrad
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115103765.png)
4. RMSProp
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115218355.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115231299.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020020911524420.png)
5. Adam优化器
结合AdaGrad和RMSProp两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115448571.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/202002091155088.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115644612.png)
由于m0初始化为0，会导致mt偏向于0，尤其在训练初期阶段。所以，此处需要对梯度均值mt进行偏差纠正，降低偏差对训练初期的影响
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115702719.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115713823.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200209115723619.png)
## sigmoid优缺点：
- 优点：
数据压缩能力，将数据规约在[0,1]之间
导数计算方便
- 缺点：
存在梯度消失问题，当x稍大的情况就接近于一条水平线
zigzag问题，非0中心化，在神经网络算法等情况下，造成反向传播时权重的全正全负的情况。

## 超参数有哪些调优方法
1. 网格搜索
2. 随机搜索
3. 贝叶斯优化算法
##  损失优化方法
问题：
1. 如何选择合适的learning_rate。自始至终保持同样的学习率显然是不太合适的，开始学习参数的时候，距离最优解比较远，需要一个较大的学习率能够快速的逼近最优解。当参数接近最优解时，继续保持最初的学习率，容易越过最优点，在最优点附近震荡。     
 2. 如何对参数选择合适的学习率。对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。