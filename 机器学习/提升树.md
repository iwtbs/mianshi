# 提升树（Boosting Tree）
[流程和实际例子](https://mp.weixin.qq.com/s/UepQi5Qezdi27MvbUSyLCA)
## 在每一轮如何改变训练数据的权值或概率分布？
AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。
## 如何将弱分类器组合成一个强分类器？
弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用
## 提升树与GBDT之间的关系？
提升树模型每一次的提升都是靠上次的预测结果与训练数据中label值的差值作为新的训练数据进行重新训练，由于原始的回归树指定了平方损失函数所以可以直接计算残差，而梯度提升决策树（Gradient Boosting Decision Tree, GDBT）针对的是一般损失函数，所以采用负梯度来近似求解残差，将残差计算替换成了损失函数的梯度方向，将上一次的预测结果带入梯度中求出本轮的训练数据。这两种模型就是在生成新的训练数据时采用了不同的方法