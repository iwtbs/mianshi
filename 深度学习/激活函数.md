# 激活函数
## sigmod
Sigmoid函数在历史上曾经非常的常用，输出值范围为[0,1]之间的实数。然而现在它已经不太受欢迎，实际中很少使用。原因是sigmoid存在3个问题：
1. sigmoid函数饱和使梯度消失
2. sigmoid函数输出不是“零为中心”
 一个多层的sigmoid神经网络，如果你的输入x都是正数，那么在反向传播中w的梯度传播到网络的某一处时，权值的变化是要么全正要么全负。
当梯度从上层传播下来，w的梯度都是用x乘以f的梯度，因此如果神经元输出的梯度是正的，那么所有w的梯度就会是正的，反之亦然。
3. 指数函数的计算是比较消耗计算资源的
## tanh
- 优点：
1.tanh解决了sigmoid的输出非“零为中心”的问题。

- 缺点：
1.依然有sigmoid函数过饱和的问题。
2.依然指数运算。
## ReLU
- 优点：
1.ReLU解决了梯度消失的问题，至少x在正区间内，神经元不会饱和。
2.由于ReLU线性、非饱和的形式，在SGD中能够快速收敛。
3.计算速度要快很多。ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。

- 缺点：
1.ReLU的输出不是“零为中心”(Notzero-centered output)。
2.随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。这种神经元的死亡是不可逆转的死亡。
## Leaky ReLU
- 优点：
1.神经元不会出现死亡的情况。
2.对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。
2.由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。
3.计算速度要快很多。Leaky ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。

- 缺点：
1.Leaky ReLU函数中的α，需要通过先验知识人工赋值。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022421200845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)