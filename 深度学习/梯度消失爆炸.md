# 神经网络梯度消失与梯度爆炸
## 简述现象
层数比较多的神经网络模型在训练的时候会出现梯度消失和梯度爆炸问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显

- 梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。
- 梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224172329770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## 产生梯度消失的根本原因
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224173209682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224173223830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
最大值是1/4。我们一般会使用标准方法来初始化网络权重，即使用一个均值为0标准差为1的高斯分布。因此，初始化的网络权值通常都小于1。层数越多，求导结果越小，最终导致梯度消失的情况出现
## 梯度爆炸的根本原因
w比较大的情况。则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题
## 当激活函数为sigmoid时，梯度消失和梯度爆炸哪个更容易发生？
梯度爆炸问题在使用sigmoid激活函数时，出现的情况较少，不容易发生。
因为要剃度爆炸就要求![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224174927128.png)，对于sigmod而言，x数值变化范围很窄
## 如何解决梯度消失和梯度爆炸
梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑以下三种方案解决：
1. 用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。
2. 用Batch Normalization。
3. LSTM的结构设计也可以改善RNN中的梯度消失问题。