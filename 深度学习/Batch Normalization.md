## 为什么传统的神经网络在训练开始之前，要对输入的数据做Normalization?
- 神经网络学习过程本质上是为了学习数据的分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；
- 另一方面，一旦在mini-batch梯度下降训练的时候，每批训练数据的分布不相同，那么网络就要在每次迭代的时候去学习以适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对所有训练数据做一个Normalization预处理的原因
## 深度神经网络模型的训练为什么会很困难
深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致后面层的输入数据分布发生变化，数据通过层层叠加的网络，靠近输出层的隐层网络输入分布变化会非常的剧烈，这就使得靠近输出层的隐层网络需要不断的去重新适应前面层的参数更新
## 什么是Batch Normalization？
难点：归一化容易，但是我们变换之后之前学习的东西就消失了
解决办法：变换重构。变换重构是为了让因训练所需而刻意加入的Normalization能够有可能还原到最初的输入
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200224213214349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
BN算法在CNN中往往放在每个卷积层之后，ReLU操作之前。在CNN中使用BN是把每个特征图看做一个神经元，计算该特征图对应数据的均值和方差进行归一化，并且每个特征图对应两个学习变量γ、β
## Batch Normalization的优点
（1）可以使用更高的学习率，BN有快速收敛的特性
（2）模型中BN可以代替dropout或者使用较低的dropout
（3）减少L2权重衰减系数。用了Batch Normalization后，可以把L2权重衰减系数降低，论文中降低为原来的5倍。
（4）BN本质上解决了反向传播过程中梯度消失的问题
因为数据使用BN后，归一化的数据仅使用了sigmoid线性的部分。
（5）可以把训练数据彻底打乱。防止了每批训练的时候，某一个样本经常被挑选到。论文中指出这个操作可以提高1%的精度