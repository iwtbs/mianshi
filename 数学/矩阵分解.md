# 矩阵分解
## 矩阵分解的作用
- 矩阵填充(通过矩阵分解来填充原有矩阵，例如协同过滤的ALS算法就是填充原有矩阵)
- 清理异常值与离群点
- 降维、压缩
- 个性化推荐
- 间接的特征组合(计算特征间相似度)
## 矩阵分解的方法
- 特征值分解。
- **PCA(Principal Component Analysis)分解，作用：降维、压缩**。
- **SVD(Singular Value Decomposition)分解，也叫奇异值分解**。
- LSI(Latent Semantic Indexing)或者叫LSA(Latent Semantic Analysis)，隐语义分析分解。
- PLSA(Probabilistic Latent Semantic Analysis)，概率潜在语义分析。PLSA和LDA都是主题模型，PLSA是判别式模型。
- NMF(Non-negative Matrix Factorization)，非负矩阵分解。非负矩阵分解能够广泛应用于图像分析、文本挖掘和语言处理等领域。
- **LDA(Latent Dirichlet Allocation)模型，潜在狄利克雷分配模型**。LDA是一种主题模型，将文档集中每篇文档的主题以概率的形式给出，可以用于主题聚类或者文本分类，是生成式模型。LDA作为主题模型可以应用到很多领域，比如：文本情感分析、文本分类、个性化推荐、社交网络、广告预测等方面。
- MF(Matrix Factorization)模型，矩阵分解模型。矩阵分解其实可以分为很多种：
- 基本矩阵分解(Basic Matrix Factorization)，basic MF分解。
- **PMF(Probabilistic Matrix Factorization)，概率矩阵分解**，主要应用到推荐系统中，在大规模的稀疏不平衡Netflix数据集上取得了较好的结果
- SVD++
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223160745771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## SVD
1. 特征值、特征向量
如果一个向量v是矩阵A的特征向量，将一定可以表示成下面的形式
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223161111642.png)
矩阵A与向量v相乘，本质上是对向量v进行了一次线性变换（旋转或拉伸），当我们求特征值与特征向量的时候，就是为了求矩阵A能使哪些向量（特征向量）只发生伸缩变换，而变换的程度可以用特征值λ表示
2. 特征值分解
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223161508462.png)
其中，Q是矩阵A的特征向量组成的矩阵，中间的矩阵则是一个对角阵，对角线上的元素就是特征值。
特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多么重要，而特征向量表示这个特征是什么。
**特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。**
3. 奇异值分解
奇异值分解是一个能适用于任意矩阵的一种分解的方法
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223162934216.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223163018655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223163328251.png)
就是我们要求的右奇异向量
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223163354733.png)
就是左奇异向量
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022316371510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
4. 工程上如何加速
两个矩阵A:m*n，B:n*p相乘，时间复杂度(O(nmp))，如果按照上面的步骤，复杂度太高了。
在奇异值分解矩阵中Σ里面的奇异值按从大到小的顺序排列，奇异值从大到小的顺序减小的特别快。在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上。也就是说，剩下的90%甚至99%的奇异值几乎没有什么作用。因此，我们可以用前面r个大的奇异值来近似描述矩阵，于是奇异值分解公式可以写成如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223163933228.png)
在奇异值分解中r的取值很重要，就是在计算精度和时间空间之间做选择。
# PCA
PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法
1. 协方差
样本X和样本Y的协方差
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223165932892.png)
协方差为正时，说明X和Y是正相关关系；协方差为负时，说明X和Y是负相关关系；协方差为0时，说明X和Y是相互独立，当样本是n维数据时，它们的协方差实际上是协方差矩阵(对称方阵)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223170353808.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
对于数据X的散度矩阵为$XX^T$。其实协方差矩阵和散度矩阵关系密切，散度矩阵就是协方差矩阵乘以（总数据量-1）。因此它们的特征值和特征向量是一样的。这里值得注意的是，散度矩阵是SVD奇异值分解的一步，因此PCA和SVD是有很大联系
2. 基于特征值分解协方差矩阵实现PCA算法
![](https://img-blog.csdnimg.cn/20200223171448726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
注意：如果我们通过特征值分解协方差矩阵，那么我们只能得到一个方向的PCA降维。这个方向就是对数据矩阵X从行(或列)方向上压缩降维
3. 基于SVD分解协方差矩阵实现PCA算法
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223171931411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
当我们用到SVD分解协方差矩阵的时候，SVD有两个好处：
（1) 有一些SVD的实现算法可以先不求出协方差矩阵也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解而是通过SVD来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是特征值分解。
（2)注意到PCA仅仅使用了我们SVD的左奇异矩阵，没有使用到右奇异值矩阵，那么右奇异值矩阵有什么用呢？
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200223172654726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)