# 内容召回
## word2vec
- 了解skip-gram和cbow两种网络的结构
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225161740893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
- 了解优化方法：Hierarchical Softmax和Negative Sampling
（1）Hierarchical Softmax
霍夫曼树，频度高的词越靠近根节点，复杂度从n降到log2n
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225161832322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
（2）负采样
每次都只是通每次都只是通过采样neg个不同的中心词做负例，就可以训练模型过采样neg个不同的中心词做负例，转化为二分类问题。
采样的词尽量是热门词
## LDA
我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。
文档到主题服从多项式分布，主题到词服从多项式分布。
**核心思想：LDA的目的就是要识别主题，即把文档—词汇矩阵变成文档—主题矩阵（分布）和主题—词汇矩阵（分布）**

对于语料库中的每篇文档，LDA定义了如下生成过程：
1.对每一篇文档，从主题分布中抽取一个主题；
2.从上述被抽到的主题所对应的单词分布中抽取一个单词；
3.重复上述过程直至遍历文档中的每一个单词。
[博客](https://www.jianshu.com/p/fa97454c9ffd)

# 行为召回
## ItemCF
- 对活跃用户进行惩罚。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225130520304.png)
- UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF的推荐结果着重于维系用户的历史兴趣
- UserCF比较适合用于新闻推荐等热门程度和实时性较强的场景，ItemCF则适用于图书、电商、电影等场景
## UserCF
- 对热门物品进行惩罚
- 在计算用户行为之间的相似度时，建立Item-User的倒排表，这样在同一个Item下面的User两两之间一定是在这个Item上有交集的，所以只需要遍历所有的Item，对其下所有的User两两进行统计即可，这样可以极大降低时间复杂度。
## 关联规则
找出用户购买的所有物品数据里频繁出现的项集活序列，来做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。如果用户购买了频繁N项集或者序列里的部分物品，那么我们可以将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括支持度，置信度和提升度等

常用的关联推荐算法有Apriori，FP Tree和PrefixSpan。如果大家不熟悉这些算法，可以参考我的另外几篇文章：
[Apriori算法](https://blog.csdn.net/qq_34219959/article/details/102381162)
[FpGrowth算法](https://blog.csdn.net/qq_34219959/article/details/102390588)
[序列模式挖掘PrefixSpan算法](https://blog.csdn.net/qq_34219959/article/details/97015246)
## 聚类协同
常用的聚类推荐算法有K-Means, BIRCH, DBSCAN和谱聚类

介绍下DBSCAN
- DBSCAN的主要优点有：
（1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。
（2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。
（3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。
- DBSCAN的主要缺点有：
（1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。
（2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
（3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响
# 矩阵分解
## 隐语义LFM
- 用两个低阶向量相乘来模拟实际的User-Item矩阵
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225130819331.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225131153230.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225131412990.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225131401259.png)
- label要怎么标注？
一般数据集里面的都是只有正向反馈，即只有标签1，这时就需要进行负采样，即采出标签0来。采样是针对每个用户来进行的，对于每个用户，负采样的Item要遵循如下原则：
（1）对每个用户，要保证正负样本的平衡(数目相似)。
（2）对每个用户采样负样本时，要选取那些很热门，而用户却没有行为的物品。
- 离线计算的空间复杂度：基于邻域的方法需要维护一张离线的相关表。假设有M个用户和N个物品，UserCF需要$O(M∗M)$的空间，ItemCF需要$O(N∗N)$的空间，而对于LFM，有F个隐类的话，需要$O(F∗(M+N))$的空间
- LFM不太适合用于物品数非常庞大的系统，如果要用，我们也需要一个比较快的算法给用户先计算一个比较小的候选列表，然后再用LFM重新排名。另一方面，LFM在生成一个用户推荐列表时速度太慢，因此不能在线实时计算，而需要离线将所有用户的推荐结果事先计算好存储在数据库中。因此，LFM不能进行在线实时推荐，也就是说，当用户有了新的行为后，他的推荐列表不会发生变化。
# 图召回
## PersonalRank
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022513152310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225131613954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
# 深度学习召回
# 因子分解排序
## FM
FM可以解决特征组合以及高维稀疏矩阵问题
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225162835212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225162909827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## FFM
FM模型中，每一个特征对应一个向量；FFM中认为每一个特征对于每一个域field对应一个向量。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225164600582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225164804771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
FFM的组合特征有10项，如下图所示
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225164814121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
其中，红色是field编号，蓝色是特征编号

# 树模型排序
## GBDT+LR
facebook采用了这个模型，树的数量《=500，每棵树的节点《=12
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225193516708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225193540660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225193555470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)

# 深度模型排序
发展历程可以参考我的文章[推荐算法—ctr预估](https://blog.csdn.net/qq_34219959/article/details/103822973)
这里只画出结构，写一些面试注意点
## DNN特征高阶组合
- 称为FNN
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225194754116.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
- 利用DNN优化高阶特征
- wide deep的深度部分就是这样的结构
## 低阶特征单独建模
- 并行结构代表：wide deep，deepfm，dcn，din
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022519534436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
- 串行结构代表：PNN，NFM，AFM
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225195735237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
## 特征交叉优化
- 原先的方式，直接dense后concat，然后传入MLP，这样交叉特征表达并不充分
- 用乘法运算体现特征交叉（PNN）。思想来源于，特征之间的关系更多的是and而非add，比如 *北京大学生* 比 *北京人或大学生*更有价值
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225200442529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
- Z是线性部分（就是embedding层的复制），P是特征交叉部分。Inner是内积运算，outer是外积运算
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225200459423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
- 计算复杂度高，采用近似计算方法，外积没有内积稳定
- 也没有低阶，只有高阶
## 小结：
- LR模型采用原始人工特征交叉
- FM自动学习xi和xj的二阶交叉特征
- PNN用内积、外积做二阶交叉
- NFM、AFM采用BI-Interaction方式学习二阶交叉
- 更高阶：DCN，任意进行特征交叉，且不增加网络参数
- DIN在embeeding层后做了一个action unit操作，对用户的兴趣分布进行学习后再输入DNN

1. Wide Deep
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225222559707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
2. Deepfm
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225222644220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
3. PNN
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022522430542.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
4. NFM
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022522462527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
5. AFM
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225224657238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
6. DCN
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225224718538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
7. DIN

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225224752423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
8. DIEN
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225224827644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
9. xDeepFM
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225224907518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
10. DSIN
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225224933683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)
11. FiBiNET 
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200225225006424.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MjE5OTU5,size_16,color_FFFFFF,t_70)



